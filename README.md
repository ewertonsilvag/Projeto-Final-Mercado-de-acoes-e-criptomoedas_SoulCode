# Projeto de Conclus√£o de Curso - Tema: Economia  üí∞ 
O projeto visou aplicar os conceitos vistos durante o curso de engenharia de dados da SoulCode para tratar, organizar e modelar os dados de no m√≠nimo 2 datasets escolhidos de acordo com o tema economia.
## Motivo dos datasets

No final de 2019 j√° podiam ser lidas not√≠cias acerca de um poss√≠vel v√≠rus em circula√ß√£o, mas s√≥ em mar√ßo de 2020 a Organiza√ß√£o Mundial da Sa√∫de (OMS) decretou pandem√≠a global pelo coranav√≠rus e uma das medidas de seguran√ßa foram o isolamento social, o que acarretou consequentemente numa desacelera√ß√£o econ√¥mica em todo o mundo. Buscando entender de que forma a economia se portou durante este per√≠odo a equipe resolveu buscar um dataset contendo informa√ß√µes sobre o mercado de a√ß√µes brasileira que contemplasse o per√≠odo de no m√≠nimo novembro de 2019 a mar√ßo de 2020 que foi o √≠nicio da pandemia. Buscando tamb√©m melhores comparativos resolvi buscar mais 2 datasets, por√©m dessa vez em outro tipo de mercado, o da criptomoedas. Os outros dois datasets escolhidos foram os das moedas Bitcoin e Ethereum. Foram realizados todos os devidos tratamentos e normaliza√ß√µes nos 3 datasets em busca de garantir insights relevantes mostrados em um dashboard simples que permitissem achar as devidas respostas para o meu questionamento inicial que era saber o comportamento do mercado econ√¥mico durante a pandemia.

## Requesitos 
-Obrigatoriamente os datasets devem ter formatos diferentes (CSV / Json / Parquet / Sql / NoSql) e 1 deles obrigatoriamente tem que ser em CSV. 

-Opera√ß√µes com Pandas (limpezas , transforma√ß√µes e normaliza√ß√µes).

-Opera√ß√µes usando PySpark com a descri√ß√£o de cada uma das opera√ß√µes.

-Opera√ß√µes utilizando o SparkSQL com a descri√ß√£o de cada umas das opera√ß√µes.

-Os datasets utilizados podem ser em lingua estrangeira , mas devem ao final terem seus dados/colunas exibidos na lingua PT-BR.

-Os datasets devem ser salvos e operados em armazenamento cloud obrigatoriamente dentro da plataforma GCP (n√£o pode ser usado Google drive ou armazenamento alheio ao google) os dados tratados devem ser armazenados tamb√©m em GCP, mas obrigatoriamente em um datalake(Gstorage ) , DW(BigQuery) ou em ambos.

-Deve ser feito an√°lises dentro do Big Query utilizando a linguagem padr√£o SQL com a descri√ß√£o das consultas feitas.

-Deve ser criado no datastudio um dash board simples para exibi√ß√£o gr√°fica dos dados tratados trazendo insights importantes.

-Deve ser demonstrado em um workflow simples (gr√°fico) as etapas de ETL.

-Implementar captura e ingest√£o de dados por meio de uma PIPELINE com modelo criado em apache beam usando o dataflow para o work.

-Criar plotagens usando pandas para alguns insights durante o processo de Transforma√ß√£o. 

-Por meio de uma PIPELINE fazer o carregamento dos dados normalizados diretamente para um DW ou DataLake ou ambos.

-Montar um relat√≥rio completo com os insights que justificam todo o processo de ETL utilizado
